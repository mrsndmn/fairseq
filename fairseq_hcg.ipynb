{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source activate fairseq_hcg_mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairseq_hcg_mha\r\n"
     ]
    }
   ],
   "source": [
    "!echo $CONDA_DEFAULT_ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data_new/d.tarasov/workspace/fairseq',\n",
       " '/home/dyutarasov/workspace/fairseq',\n",
       " '/opt/software/python/anaconda/2019_10/lib/python37.zip',\n",
       " '/opt/software/python/anaconda/2019_10/lib/python3.7',\n",
       " '/opt/software/python/anaconda/2019_10/lib/python3.7/lib-dynload',\n",
       " '',\n",
       " '/opt/software/python/anaconda/2019_10/lib/python3.7/site-packages',\n",
       " '/opt/software/python/anaconda/2019_10/lib/python3.7/site-packages/IPython/extensions',\n",
       " '/home/dyutarasov/.ipython']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "fairseq_path = '/data_new/d.tarasov/workspace/fairseq'\n",
    "if fairseq_path not in sys.path:\n",
    "    sys.path = [ fairseq_path ] + sys.path\n",
    "\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 60437228\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084727 окт 12 23:30 checkpoint100.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 10:59 checkpoint100.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084727 окт 13 01:02 checkpoint101.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 10:59 checkpoint101.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084091 окт 13 02:19 checkpoint_102_525000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084091 окт 13 02:35 checkpoint102.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 10:59 checkpoint102.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084727 окт 13 04:06 checkpoint103.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 10:59 checkpoint103.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084727 окт 13 05:39 checkpoint104.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 11:00 checkpoint104.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084727 окт 13 07:11 checkpoint105.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 11:00 checkpoint105.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084091 окт 13 07:38 checkpoint_106_550000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084727 окт 13 08:43 checkpoint106.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 11:00 checkpoint106.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084727 окт 13 10:14 checkpoint107.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 11:00 checkpoint107.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084727 окт 13 11:47 checkpoint108.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 18:23 checkpoint108.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084155 окт 13 12:58 checkpoint_109_575000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084791 окт 13 13:19 checkpoint109.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 18:23 checkpoint109.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084791 окт 13 14:52 checkpoint110.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 18:23 checkpoint110.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084791 окт 13 16:24 checkpoint111.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 18:23 checkpoint111.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084791 окт 13 17:56 checkpoint112.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 13 18:23 checkpoint112.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084155 окт 13 18:18 checkpoint_113_600000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952098659 окт 10 23:11 checkpoint61.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 10 23:33 checkpoint61.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952098659 окт 11 00:00 checkpoint62.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 08:53 checkpoint62.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952098659 окт 11 00:51 checkpoint63.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 08:53 checkpoint63.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952098659 окт 11 01:41 checkpoint64.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 08:54 checkpoint64.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952098659 окт 11 02:31 checkpoint65.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 08:54 checkpoint65.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952098659 окт 11 03:21 checkpoint66.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 08:54 checkpoint66.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952095229 окт 11 03:43 checkpoint_67_325000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952098659 окт 11 04:12 checkpoint67.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 08:54 checkpoint67.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952098659 окт 11 05:03 checkpoint68.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 08:54 checkpoint68.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952098659 окт 11 05:54 checkpoint69.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 08:54 checkpoint69.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952098659 окт 11 06:44 checkpoint70.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 08:54 checkpoint70.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 10:13 checkpoint71.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 11:13 checkpoint71.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 11:03 checkpoint72.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 11:13 checkpoint72.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952095929 окт 11 11:33 checkpoint_73_350000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 11:54 checkpoint73.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 15:00 checkpoint73.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 12:47 checkpoint74.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 15:00 checkpoint74.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 13:38 checkpoint75.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 15:00 checkpoint75.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 14:29 checkpoint76.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 15:00 checkpoint76.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 15:19 checkpoint77.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 19:22 checkpoint77.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 16:10 checkpoint78.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 19:22 checkpoint78.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952095929 окт 11 16:47 checkpoint_79_375000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 17:01 checkpoint79.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 19:22 checkpoint79.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 17:51 checkpoint80.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 19:22 checkpoint80.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 18:42 checkpoint81.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 19:22 checkpoint81.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 19:32 checkpoint82.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 21:16 checkpoint82.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 20:23 checkpoint83.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 21:16 checkpoint83.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952099423 окт 11 21:13 checkpoint84.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 11 21:16 checkpoint84.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952083899 окт 11 22:48 checkpoint_85_400000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 11 23:34 checkpoint85.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 08:19 checkpoint85.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 01:07 checkpoint86.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 08:19 checkpoint86.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 02:40 checkpoint87.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 08:19 checkpoint87.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952083899 окт 12 04:11 checkpoint_88_425000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 04:14 checkpoint88.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 08:19 checkpoint88.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 05:47 checkpoint89.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 08:19 checkpoint89.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 07:21 checkpoint90.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 08:19 checkpoint90.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 08:55 checkpoint91.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 14:53 checkpoint91.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952083899 окт 12 09:35 checkpoint_92_450000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 10:27 checkpoint92.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 14:54 checkpoint92.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 11:59 checkpoint93.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 14:54 checkpoint93.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 13:31 checkpoint94.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 14:54 checkpoint94.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952083899 окт 12 14:55 checkpoint_95_475000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 15:05 checkpoint95.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 19:57 checkpoint95.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 16:36 checkpoint96.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 19:57 checkpoint96.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 18:07 checkpoint97.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 19:58 checkpoint97.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 19:39 checkpoint98.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 19:58 checkpoint98.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952083899 окт 12 20:13 checkpoint_99_500000.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users 952084535 окт 12 21:11 checkpoint99.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users      1396 окт 12 23:17 checkpoint99.pt.mhas_probas.pickle\r\n",
      "-rw-r--r-- 1 dyutarasov users 952085047 окт 13 21:40 checkpoint_last.pt\r\n",
      "-rw-r--r-- 1 dyutarasov users    317899 окт 15 02:41 l0.1_decoder_encoder_attention_p_open.gif\r\n",
      "-rw-r--r-- 1 dyutarasov users     20902 окт 15 02:41 l0.1_decoder_encoder_attention_p_open_last.png\r\n",
      "-rw-r--r-- 1 dyutarasov users    522440 окт 15 02:40 l0.1_decoder_self_attention_p_open.gif\r\n",
      "-rw-r--r-- 1 dyutarasov users     20296 окт 15 02:40 l0.1_decoder_self_attention_p_open_last.png\r\n",
      "-rw-r--r-- 1 dyutarasov users    318962 окт 15 02:39 l0.1_encoder_attention_p_open.gif\r\n",
      "-rw-r--r-- 1 dyutarasov users     19867 окт 15 02:39 l0.1_encoder_attention_p_open_last.png\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l checkpoints_nonautoregressive_transformer_hcg_pruning_lambda_0.1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/software/python/anaconda/2019_10/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'omegaconf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7291db4ce74b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfairseq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheckpoint_utils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/workspace/fairseq/fairseq/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# backwards compatibility to support `from fairseq.X import Y`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfairseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdistributed_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfairseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmeters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/fairseq/fairseq/distributed/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributed_timeout_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedTimeoutWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfully_sharded_data_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfsdp_enable_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfsdp_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFullyShardedDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlegacy_distributed_data_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLegacyDistributedDataParallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmodule_proxy_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModuleProxyWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/fairseq/fairseq/distributed/fully_sharded_data_parallel.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfairseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDistributedTrainingConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfairseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/fairseq/fairseq/dataclass/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# LICENSE file in the root directory of this source tree.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfigs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFairseqDataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChoiceEnum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/fairseq/fairseq/dataclass/configs.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0momegaconf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mII\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMISSING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'omegaconf'"
     ]
    }
   ],
   "source": [
    "from fairseq import checkpoint_utils, options, tasks, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-929b5ecd2541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'checkpoints_nonautoregressive_transformer_hcg_pruning_lambda_1.5/checkpoint_last.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_paths\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint_utils' is not defined"
     ]
    }
   ],
   "source": [
    "checkpoint_path = ''aml'\n",
    "model = checkpoint_utils.load_model_ensemble(utils.split_paths( checkpoint_path ))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14, 160, 160])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.repeat_interleave( torch.unsqueeze(torch.zeros((14, 160)), -1), 160, dim=2 ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairseq.modules import MultiheadAttention\n",
    "from fairseq.modules.hcg_attention import MultiHeadHCGAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mha(mha_class):\n",
    "    \n",
    "    hidd_dim = 64\n",
    "    num_heads = 8\n",
    "\n",
    "    mha = mha_class(hidd_dim, num_heads)\n",
    "\n",
    "    qkv = torch.rand((7, 8, hidd_dim))\n",
    "    att_res, _ = mha.forward(qkv, qkv, qkv)\n",
    "    \n",
    "    att_res.shape == qkv.shape\n",
    "    \n",
    "    return att_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mha(MultiHeadHCGAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(module):\n",
    "    return sum( p.numel() for p in module.parameters() if p.requires_grad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16640"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha_hid_dim = 64\n",
    "mha_num_heads = 8\n",
    "\n",
    "mha = MultiheadAttention(mha_hid_dim, mha_num_heads)\n",
    "mha.dropout_module.p = 0\n",
    "count_trainable_params(mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({torch.Size([64, 64]): 4, torch.Size([64]): 4})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter( p.shape for p in mha.parameters() if p.requires_grad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({torch.Size([8, 64]): 24,\n",
       "         torch.Size([8]): 24,\n",
       "         torch.Size([64, 64]): 1,\n",
       "         torch.Size([64]): 1})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter( p.shape for p in hcg_mha.parameters() if p.requires_grad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (dropout_module): FairseqDropout()\n",
       "  (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadHCGAttention(\n",
       "  (attention_heads): ModuleList(\n",
       "    (0): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (1): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (2): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (3): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (4): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (5): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (6): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (7): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (hard_concrete_gates): ModuleList()\n",
       "  (heads_weights): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hcg_mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16640"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hcg_mha = MultiHeadHCGAttention(mha_hid_dim, mha_num_heads)\n",
    "count_trainable_params(hcg_mha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadHCGAttention(\n",
       "  (attention_heads): ModuleList(\n",
       "    (0): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (1): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (2): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (3): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (4): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (5): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (6): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "    (7): HCGAttention(\n",
       "      (query_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (key_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (value_weights): Linear(in_features=64, out_features=8, bias=True)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (hard_concrete_gates): ModuleList()\n",
       "  (heads_weights): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hcg_mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4929, -0.2564,  0.0943,  ...,  0.3369,  0.5001,  0.3237],\n",
       "         [-0.5830, -0.2702,  0.3887,  ...,  0.2738,  0.6114,  0.3171],\n",
       "         [-0.5882, -0.1335,  0.3095,  ...,  0.2822,  0.6535,  0.1725],\n",
       "         ...,\n",
       "         [-0.5204, -0.2848,  0.3575,  ...,  0.1799,  0.4535,  0.1451],\n",
       "         [-0.6387, -0.1075,  0.4367,  ...,  0.4107,  0.5984,  0.1606],\n",
       "         [-0.3994, -0.2388,  0.3155,  ...,  0.2539,  0.5598,  0.0820]],\n",
       "\n",
       "        [[-0.4919, -0.2583,  0.0970,  ...,  0.3367,  0.4935,  0.3277],\n",
       "         [-0.5808, -0.2730,  0.3884,  ...,  0.2778,  0.6175,  0.3167],\n",
       "         [-0.5850, -0.1261,  0.3159,  ...,  0.2793,  0.6603,  0.1727],\n",
       "         ...,\n",
       "         [-0.5182, -0.2839,  0.3581,  ...,  0.1781,  0.4549,  0.1379],\n",
       "         [-0.6355, -0.1100,  0.4374,  ...,  0.4108,  0.5951,  0.1594],\n",
       "         [-0.3985, -0.2439,  0.3257,  ...,  0.2502,  0.5583,  0.0751]],\n",
       "\n",
       "        [[-0.4975, -0.2561,  0.0931,  ...,  0.3369,  0.4989,  0.3251],\n",
       "         [-0.5760, -0.2723,  0.3966,  ...,  0.2842,  0.6200,  0.3194],\n",
       "         [-0.5926, -0.1339,  0.3166,  ...,  0.2825,  0.6542,  0.1725],\n",
       "         ...,\n",
       "         [-0.5101, -0.2793,  0.3641,  ...,  0.1773,  0.4611,  0.1366],\n",
       "         [-0.6363, -0.1123,  0.4430,  ...,  0.4128,  0.5953,  0.1595],\n",
       "         [-0.3988, -0.2459,  0.3118,  ...,  0.2487,  0.5679,  0.0812]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.4934, -0.2556,  0.0931,  ...,  0.3395,  0.5020,  0.3270],\n",
       "         [-0.5812, -0.2709,  0.3799,  ...,  0.2741,  0.6187,  0.3159],\n",
       "         [-0.5846, -0.1331,  0.3176,  ...,  0.2769,  0.6534,  0.1696],\n",
       "         ...,\n",
       "         [-0.5151, -0.2814,  0.3600,  ...,  0.1793,  0.4550,  0.1418],\n",
       "         [-0.6399, -0.1085,  0.4361,  ...,  0.4108,  0.5945,  0.1624],\n",
       "         [-0.4009, -0.2466,  0.3177,  ...,  0.2534,  0.5653,  0.0862]],\n",
       "\n",
       "        [[-0.4914, -0.2530,  0.0927,  ...,  0.3388,  0.4940,  0.3236],\n",
       "         [-0.5783, -0.2706,  0.3885,  ...,  0.2738,  0.6121,  0.3165],\n",
       "         [-0.5878, -0.1314,  0.3159,  ...,  0.2716,  0.6608,  0.1712],\n",
       "         ...,\n",
       "         [-0.5086, -0.2774,  0.3641,  ...,  0.1814,  0.4576,  0.1380],\n",
       "         [-0.6368, -0.1079,  0.4433,  ...,  0.4136,  0.5974,  0.1601],\n",
       "         [-0.3953, -0.2464,  0.3175,  ...,  0.2459,  0.5701,  0.0778]],\n",
       "\n",
       "        [[-0.4912, -0.2520,  0.0951,  ...,  0.3375,  0.5004,  0.3234],\n",
       "         [-0.5798, -0.2738,  0.3887,  ...,  0.2781,  0.6134,  0.3161],\n",
       "         [-0.5836, -0.1262,  0.3122,  ...,  0.2814,  0.6538,  0.1728],\n",
       "         ...,\n",
       "         [-0.5128, -0.2771,  0.3628,  ...,  0.1779,  0.4561,  0.1323],\n",
       "         [-0.6393, -0.1091,  0.4420,  ...,  0.4098,  0.5954,  0.1607],\n",
       "         [-0.4019, -0.2505,  0.3161,  ...,  0.2506,  0.5640,  0.0837]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mha(MultiheadAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = torch.rand((3, 64, 8)), torch.rand((3, 8, 64))\n",
    "\n",
    "\n",
    "torch.bmm(a, b) == torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
